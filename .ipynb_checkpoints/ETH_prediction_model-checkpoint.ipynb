{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook I include all relevant functions used in this model\n",
    "# The goal is to predict the upward/downward of the Ethereum cryptocurrency on a 3-minute time horizon\n",
    "# The use of each function is described throroughly throughout the notebook\n",
    "# I use Gemini exchange data available at: https://www.cryptodatadownload.com/data/gemini/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets used for this model can be found in the /crypto_data/Gemini folder\n",
    "# Each dataset includes minute-by-minute open, high, low, close prices of a particular crypto - BTC, ETH, LTC, ZEC \n",
    "# Datasets are loaded into pandas dataframes for further manipulation\n",
    "\n",
    "# The below function creates a combined dataset of both ETH and BTC price data from a given year (YEAR)\n",
    "\n",
    "def load_gemini_dateset(YEAR):\n",
    "    \n",
    "    # Loading datasets\n",
    "    \n",
    "    btc_usd = pd.read_csv(f\"crypto_data/Gemini/BTC-USD_{YEAR}.csv\", names=[\"time\", \"date\", \"symbol\", \"btc_open\", \"btc_high\", \"btc_low\", \"btc_close\", \"btc_volume\"])\n",
    "    eth_usd = pd.read_csv(f\"crypto_data/Gemini/ETH-USD_{YEAR}.csv\", names=[\"time\", \"date\", \"symbol\", \"eth_open\", \"eth_high\", \"eth_low\", \"eth_close\", \"eth_volume\"])\n",
    "    \n",
    "    # This function fixes the inconsistent timestamps within datasets\n",
    "    \n",
    "    def unix_to_seconds(time):\n",
    "        if time > 1900000000:\n",
    "            return time // 1000\n",
    "        else:\n",
    "            return time\n",
    "\n",
    "    # Both datasets are pruned, rearranged and their time column is transoformed\n",
    "    \n",
    "    btc_usd.drop([\"date\", \"symbol\"], axis=1, inplace=True)\n",
    "    btc_usd = btc_usd[::-1]\n",
    "    btc_usd.reset_index(drop=True, inplace=True)\n",
    "    btc_usd.time = btc_usd.time.apply(unix_to_seconds)\n",
    "    \n",
    "    eth_usd.drop([\"date\", \"symbol\"], axis=1, inplace=True)\n",
    "    eth_usd = eth_usd[::-1]\n",
    "    eth_usd.reset_index(drop=True, inplace=True)\n",
    "    eth_usd.time = eth_usd.time.apply(unix_to_seconds)\n",
    "    \n",
    "    # Both datasets are merged on time and missing values are filled with the ones preceding them\n",
    "    # In most instances gaps were sporadic and not a large proportion has been filled\n",
    "\n",
    "    df = pd.merge(eth_usd, btc_usd, how='left', on='time')\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.iloc[1:]\n",
    "        \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>eth_open</th>\n",
       "      <th>eth_high</th>\n",
       "      <th>eth_low</th>\n",
       "      <th>eth_close</th>\n",
       "      <th>eth_volume</th>\n",
       "      <th>btc_open</th>\n",
       "      <th>btc_high</th>\n",
       "      <th>btc_low</th>\n",
       "      <th>btc_close</th>\n",
       "      <th>btc_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>1515082860</td>\n",
       "      <td>981.51</td>\n",
       "      <td>981.81</td>\n",
       "      <td>981.50</td>\n",
       "      <td>981.54</td>\n",
       "      <td>138.218681</td>\n",
       "      <td>14514.50</td>\n",
       "      <td>14521.00</td>\n",
       "      <td>14514.49</td>\n",
       "      <td>14515.81</td>\n",
       "      <td>5.469575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>1515082920</td>\n",
       "      <td>981.54</td>\n",
       "      <td>981.83</td>\n",
       "      <td>981.00</td>\n",
       "      <td>981.00</td>\n",
       "      <td>76.948456</td>\n",
       "      <td>14515.81</td>\n",
       "      <td>14521.00</td>\n",
       "      <td>14514.49</td>\n",
       "      <td>14514.68</td>\n",
       "      <td>4.081804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>1515082980</td>\n",
       "      <td>981.00</td>\n",
       "      <td>981.50</td>\n",
       "      <td>981.00</td>\n",
       "      <td>981.49</td>\n",
       "      <td>76.140935</td>\n",
       "      <td>14514.68</td>\n",
       "      <td>14530.49</td>\n",
       "      <td>14514.49</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>27.435541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>1515083040</td>\n",
       "      <td>981.49</td>\n",
       "      <td>981.49</td>\n",
       "      <td>981.00</td>\n",
       "      <td>981.49</td>\n",
       "      <td>236.583591</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>14515.01</td>\n",
       "      <td>14515.01</td>\n",
       "      <td>0.454036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>1515083100</td>\n",
       "      <td>981.49</td>\n",
       "      <td>983.00</td>\n",
       "      <td>981.49</td>\n",
       "      <td>983.00</td>\n",
       "      <td>0.660766</td>\n",
       "      <td>14515.01</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>14515.01</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>1.569043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>1515083160</td>\n",
       "      <td>983.00</td>\n",
       "      <td>983.57</td>\n",
       "      <td>983.00</td>\n",
       "      <td>983.56</td>\n",
       "      <td>14.883745</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>14558.96</td>\n",
       "      <td>14520.98</td>\n",
       "      <td>14534.55</td>\n",
       "      <td>5.380189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5307</th>\n",
       "      <td>1515083220</td>\n",
       "      <td>983.56</td>\n",
       "      <td>984.45</td>\n",
       "      <td>983.56</td>\n",
       "      <td>984.29</td>\n",
       "      <td>4.482341</td>\n",
       "      <td>14534.55</td>\n",
       "      <td>14551.75</td>\n",
       "      <td>14534.55</td>\n",
       "      <td>14536.59</td>\n",
       "      <td>0.152562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5308</th>\n",
       "      <td>1515083280</td>\n",
       "      <td>984.29</td>\n",
       "      <td>984.29</td>\n",
       "      <td>984.24</td>\n",
       "      <td>984.25</td>\n",
       "      <td>32.180429</td>\n",
       "      <td>14536.59</td>\n",
       "      <td>14536.59</td>\n",
       "      <td>14530.49</td>\n",
       "      <td>14536.59</td>\n",
       "      <td>2.374266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5309</th>\n",
       "      <td>1515083340</td>\n",
       "      <td>984.25</td>\n",
       "      <td>984.25</td>\n",
       "      <td>984.24</td>\n",
       "      <td>984.24</td>\n",
       "      <td>13.553575</td>\n",
       "      <td>14536.59</td>\n",
       "      <td>14559.84</td>\n",
       "      <td>14534.93</td>\n",
       "      <td>14546.45</td>\n",
       "      <td>1.582403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>1515083400</td>\n",
       "      <td>984.24</td>\n",
       "      <td>984.24</td>\n",
       "      <td>984.01</td>\n",
       "      <td>984.01</td>\n",
       "      <td>29.567840</td>\n",
       "      <td>14546.45</td>\n",
       "      <td>14546.47</td>\n",
       "      <td>14545.01</td>\n",
       "      <td>14545.01</td>\n",
       "      <td>4.083873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time  eth_open  eth_high  eth_low  eth_close  eth_volume  \\\n",
       "5301  1515082860    981.51    981.81   981.50     981.54  138.218681   \n",
       "5302  1515082920    981.54    981.83   981.00     981.00   76.948456   \n",
       "5303  1515082980    981.00    981.50   981.00     981.49   76.140935   \n",
       "5304  1515083040    981.49    981.49   981.00     981.49  236.583591   \n",
       "5305  1515083100    981.49    983.00   981.49     983.00    0.660766   \n",
       "5306  1515083160    983.00    983.57   983.00     983.56   14.883745   \n",
       "5307  1515083220    983.56    984.45   983.56     984.29    4.482341   \n",
       "5308  1515083280    984.29    984.29   984.24     984.25   32.180429   \n",
       "5309  1515083340    984.25    984.25   984.24     984.24   13.553575   \n",
       "5310  1515083400    984.24    984.24   984.01     984.01   29.567840   \n",
       "\n",
       "      btc_open  btc_high   btc_low  btc_close  btc_volume  \n",
       "5301  14514.50  14521.00  14514.49   14515.81    5.469575  \n",
       "5302  14515.81  14521.00  14514.49   14514.68    4.081804  \n",
       "5303  14514.68  14530.49  14514.49   14520.98   27.435541  \n",
       "5304  14520.98  14520.98  14515.01   14515.01    0.454036  \n",
       "5305  14515.01  14520.98  14515.01   14520.98    1.569043  \n",
       "5306  14520.98  14558.96  14520.98   14534.55    5.380189  \n",
       "5307  14534.55  14551.75  14534.55   14536.59    0.152562  \n",
       "5308  14536.59  14536.59  14530.49   14536.59    2.374266  \n",
       "5309  14536.59  14559.84  14534.93   14546.45    1.582403  \n",
       "5310  14546.45  14546.47  14545.01   14545.01    4.083873  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A snapshot of the dataframe data is shown below:\n",
    "\n",
    "YEAR = 2018\n",
    "\n",
    "df = load_gemini_dateset(YEAR)\n",
    "df.iloc[5300:5310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# The function below calassifies each minute on a upward (1) / downward (0) ETH movement basis\n",
    "# We specify long in advance do we want to predict - in our case this is 3 minutes\n",
    "# Next, we specify how many minutes back is the model supposed to look - this will be 45 minutes\n",
    "# TARGET variable specifies the feature we are trying to predict - e.g. eth_close\n",
    "\n",
    "def preprocessing(df, TIME_BACK, TIME_IN_ADVANCE, TARGET, YEAR):\n",
    "    \n",
    "    # TIME_BACK        - how many minutes into the past is the model looking\n",
    "    # TIME_IN_ADVANCE  - how many minutes into the future are we trying to predict\n",
    "    # TARGET           - the exchange rate we are trying to predict\n",
    "    \n",
    "    # Appending a column with a future value of a target exchange\n",
    "    n = TIME_BACK + 2 # 2 comes from further data preprocessing\n",
    "    \n",
    "    # Filling the label with NANs initially\n",
    "    df[f\"future_{TARGET}\"] = np.nan\n",
    "    \n",
    "    # Iterating through each row of the dataset\n",
    "    while n + TIME_IN_ADVANCE < len(df):\n",
    "        \n",
    "        # Check if the value in advance is present in the df\n",
    "        if df['time'].iloc[n + TIME_IN_ADVANCE] - TIME_IN_ADVANCE * 60 == df['time'].iloc[n]:\n",
    "            \n",
    "            # Check if all previous values back in time are present\n",
    "            if len(df['time'].iloc[n - TIME_BACK : n].diff().value_counts()) == 1:\n",
    "                \n",
    "                # Assign future value\n",
    "                df.at[n, f'future_{TARGET}'] = df[TARGET].iloc[n + TIME_IN_ADVANCE] \n",
    "                \n",
    "        n += 1\n",
    "        \n",
    "        print(f\"{n} out of {len(df)} rows\", end='\\r')\n",
    "        \n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    # Adding the future_change column describing the actual change from the present value\n",
    "    # Adding a target value 1 - goes up, 0 - goes down\n",
    "    df['future_change'] = df[f'future_{TARGET}'] / df[TARGET[:3] + '_open']\n",
    "    df['target'] = df[\"future_change\"].apply(classify)\n",
    "\n",
    "    # Scaling and preprocessing feature variables\n",
    "    df = scale_feature_variables(df, TARGET)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Function used to fill the target column in the dataframe\n",
    "\n",
    "def classify(future_change):\n",
    "    if future_change > 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# Each feature is scaled to improve the learning process of the network\n",
    "    \n",
    "    \n",
    "def scale_feature_variables(df, TARGET):\n",
    "    target_df = df.copy()\n",
    "    columns = target_df.columns\n",
    "    for col in columns:\n",
    "        if col not in [\"time\", f\"future_{TARGET}\", \"future_change\", \"target\"]:\n",
    "            target_df[col] = target_df[col].pct_change()\n",
    "            target_df[col] = target_df[col].replace([np.inf], 1)\n",
    "            target_df[col] = scale(list(target_df[col]))\n",
    "           \n",
    "    return target_df.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that transforms the preprocessed dataframe into a 3-D feature tensor X of shape [no_samples, TIME_BACK, no_features]\n",
    "# It also returns 1-D label tensor y\n",
    "\n",
    "def prepare_features(df, TARGET, TIME_BACK, YEAR):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    rows = []\n",
    "    \n",
    "    # Iterating over every minute of the dataset\n",
    "    \n",
    "    for i in range(TIME_BACK, len(df)):\n",
    "        \n",
    "        # Check if the future value exist\n",
    "        if np.isnan(df[f'future_change'].iloc[i]) or df[f'future_change'].iloc[i] == 1.0:\n",
    "            continue\n",
    "        \n",
    "        # Check if all feature variables are present\n",
    "        if df.iloc[i-TIME_BACK:i, 1:-3].isnull().values.any():\n",
    "            continue\n",
    "            \n",
    "        # Create a sample and add it to the feature and label array\n",
    "        X.append(np.array(df.iloc[i-TIME_BACK:i, 1:-3]))\n",
    "        y.append(df['target'].iloc[i])\n",
    "        rows.append(i)\n",
    "        \n",
    "        print(f\"{i} out of {len(df)} rows\", end='\\r')\n",
    "    print(\"\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below splits the data into train and test batches\n",
    "\n",
    "def split_train_test_data(X, y, split_point=75000):\n",
    "    \n",
    "    # Split the data into training and testing, and shuffle the train part\n",
    "    \n",
    "    X_train, X_test = X[:split_point], X[split_point:]\n",
    "    y_train, y_test = y[:split_point], y[split_point:]\n",
    "    train = list(zip(X_train, y_train))\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    # Recover the X and y train and reshape them for the input layer of the network\n",
    "    # Make sure y labels are of type int\n",
    "    \n",
    "    X_train, y_train = np.array(train)[:, 0], np.array(train)[:, 1].astype(int)\n",
    "    X_train = np.rollaxis(np.dstack(X_train), -1)\n",
    "    X_test = np.rollaxis(np.dstack(X_test), -1)\n",
    "    y_test = np.array(y_test).astype(int)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order for the training process to be effective, and not end up at a sub-optimal minimum,\n",
    "# the training data need to be balanced 50-50 between upward (1) and downward (0) classes\n",
    "# The below function balances the training sample\n",
    "\n",
    "def balance_the_train_batch(X_train, y_train):\n",
    "\n",
    "    # Dividing feature samples into the ones associated with different classes\n",
    "    \n",
    "    positive_X = []\n",
    "    negative_X = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i] == 0:\n",
    "            negative_X.append(X_train[i])\n",
    "        elif y_train[i] == 1:\n",
    "            positive_X.append(X_train[i])\n",
    "            \n",
    "    # Limiting the number of majority class instance to the number of minority occurances\n",
    "    \n",
    "    if len(positive_X) > len(negative_X):\n",
    "        positive_X = positive_X[:len(negative_X)]\n",
    "        \n",
    "    elif len(positive_X) < len(negative_X):\n",
    "        negative_X = negative_X[:len(positive_X)]\n",
    "        \n",
    "        \n",
    "    # Zipping all feature samples back together and shuffling them to avoid generalizations in the training process\n",
    "    \n",
    "    train = list(zip(positive_X, np.ones(len(positive_X)))) + list(zip(negative_X, np.zeros(len(negative_X))))\n",
    "    random.shuffle(train)\n",
    "    \n",
    "    # Changing the output shape to fit the network\n",
    "    \n",
    "    X_train, y_train = np.array(train)[:, 0], np.array(train)[:, 1].astype(int)\n",
    "    X_train = np.rollaxis(np.dstack(X_train), -1)\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data using function specified above\n",
    "\n",
    "YEAR = 2018\n",
    "TARGET = \"eth_close\"\n",
    "TIME_BACK = 45\n",
    "TIME_IN_ADVANCE = 3\n",
    "\n",
    "df = load_gemini_dateset(YEAR)\n",
    "df = preprocessing(df, TIME_BACK, TIME_IN_ADVANCE, TARGET, YEAR)\n",
    "X, y = prepare_features(df, TARGET, TIME_BACK, YEAR)\n",
    "X_train, X_test, y_train, y_test = split_train_test_data(X, y, split_point=75000)\n",
    "X_train, y_train = balance_the_train_batch(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I implement the TensorFlow library to create a network architecture that will suit the problem\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "\n",
    "NAME = f\"Gemini-3-LSTM-3-Dense-TIMEBACK-{TIME_BACK}-TARGET-{TARGET}-{time.time()}\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10   # Low epoch number is caused by a huge number of samples given in each epoch\n",
    "\n",
    "# Below the architecture of the neural network is specified\n",
    "# I mostly test a mixture of LSTM and Dense layers with using Dropout regularization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# I record the learning process using the TensorBoard library\n",
    "# I save the model after each epoch in the /models folder\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f\"logs\\{NAME}\")\n",
    "\n",
    "filepath = NAME + \"\\{epoch:02d}-{val_accuracy:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models\\{}.model\".format(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Choose the model you want to evaluate, and print the performance of the model after each epoch\n",
    "\n",
    "MODEL_NAME = \"Gemini-4-LSTM-2-Dense-TIMEBACK-45-TARGET-eth_close-1581412893.36031\"\n",
    "\n",
    "for filename in os.listdir(fr\"models\\{MODEL_NAME}\"):\n",
    "\n",
    "    if not filename.endswith(\".model\"):\n",
    "        continue\n",
    "\n",
    "    # Get probability distributions returned by the network and change them to class predictions\n",
    "    \n",
    "    model = tf.keras.models.load_model(f\"models\\{MODEL_NAME}\\{filename}\")\n",
    "    distributions = model.predict(X_test)\n",
    "    predictions = []\n",
    "    \n",
    "    for dist in distributions:\n",
    "        if dist[0] > dist[1]:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "            \n",
    "    # Print the loss, accuracy and confusion matrix of the model at each epoch\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    print(confusion_matrix(y_test, predictions), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
